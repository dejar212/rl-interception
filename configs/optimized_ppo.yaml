# Оптимизированная конфигурация PPO для 1M timesteps
# Исправлены проблемы предыдущей версии

algorithm:
  name: "PPO"
  
  # Значительно увеличенная архитектура
  hidden_dim: 512                   # Увеличено с 256 до 512 для лучшего обучения
  
  # Гиперпараметры обучения - оптимизированы для длительного обучения
  learning_rate: 0.0001             # Уменьшен для более стабильного обучения
  gamma: 0.99
  gae_lambda: 0.95
  
  # PPO специфичные параметры
  clip_coef: 0.2
  vf_coef: 0.5
  ent_coef: 0.02                    # Увеличена энтропия для лучшего exploration
  max_grad_norm: 0.5
  
  # КЛЮЧЕВЫЕ параметры обучения (теперь используются из конфига!)
  total_timesteps: 1000000          
  num_steps: 4096                   # УВЕЛИЧЕНО с 2048 - больше данных на rollout
  batch_size: 256                   # УВЕЛИЧЕНО с 64 - более стабильное обучение
  num_epochs: 20                    # УВЕЛИЧЕНО с 10 - лучше используем собранные данные
  
  device: "cpu"


