#!/usr/bin/env python3
"""
–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∏–º–∞—Ü–∏–π –ª—É—á—à–∏—Ö –∏ —Ö—É–¥—à–∏—Ö —Å–ª—É—á–∞–µ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
"""
import sys
sys.path.append('.')

import json
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from pathlib import Path
from typing import Dict, List
import argparse

from environment import InterceptionEnv
from algorithms import PPOAgent, SACAgent, TD3Agent, DQNAgent, A2CAgent, DDPGAgent
from utils import set_seed


class BestWorstAnimator:
    """–°–æ–∑–¥–∞—Ç–µ–ª—å –∞–Ω–∏–º–∞—Ü–∏–π –ª—É—á—à–∏—Ö –∏ —Ö—É–¥—à–∏—Ö –∏—Å—Ö–æ–¥–æ–≤"""
    
    def __init__(self, models_info: Dict, output_dir: Path):
        self.models_info = models_info
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.agent_classes = {
            'ppo': PPOAgent,
            'sac': SACAgent,
            'td3': TD3Agent,
            'dqn': DQNAgent,
            'a2c': A2CAgent,
            'ddpg': DDPGAgent
        }
    
    def load_agent(self, algo_name: str, model_path: str, config_path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∞–≥–µ–Ω—Ç–∞"""
        import yaml
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
        env = InterceptionEnv()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
        agent_class = self.agent_classes[algo_name]
        agent = agent_class(env, config['algorithm'])
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        agent.load(model_path)
        
        return agent, env
    
    def find_best_worst_scenarios(self, algo_name: str, n_scenarios: int = 10):
        """–ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –∏ —Ö—É–¥—à–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞"""
        print(f"\nüîç Finding best and worst scenarios for {algo_name.upper()}...")
        
        model_path = self.models_info[algo_name]['model_path']
        config_path = self.models_info[algo_name]['algo_config']
        
        agent, env = self.load_agent(algo_name, model_path, config_path)
        
        scenarios = []
        
        for episode in range(n_scenarios):
            set_seed(42 + episode)
            obs, _ = env.reset()
            
            episode_reward = 0
            done = False
            steps = 0
            
            while not done and steps < 500:
                action = agent.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                episode_reward += reward
                steps += 1
            
            success = info.get('success', False)
            collision = info.get('collision', False)
            
            scenarios.append({
                'seed': 42 + episode,
                'reward': episode_reward,
                'success': success,
                'collision': collision,
                'steps': steps
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–∞–≥—Ä–∞–¥–µ
        scenarios.sort(key=lambda x: x['reward'], reverse=True)
        
        best = scenarios[0]
        worst = scenarios[-1]
        
        print(f"  ‚úÖ Best: Reward={best['reward']:.2f}, Success={best['success']}, Steps={best['steps']}")
        print(f"  ‚ùå Worst: Reward={worst['reward']:.2f}, Success={worst['success']}, Steps={worst['steps']}")
        
        return best, worst
    
    def create_animation(self, algo_name: str, seed: int, title: str, output_file: Path):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∏–º–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è"""
        print(f"üé¨ Creating animation: {output_file.name}")
        
        model_path = self.models_info[algo_name]['model_path']
        config_path = self.models_info[algo_name]['algo_config']
        
        agent, env = self.load_agent(algo_name, model_path, config_path)
        
        # –ó–∞–ø—É—Å–∫ —ç–ø–∏–∑–æ–¥–∞
        set_seed(seed)
        obs, _ = env.reset()
        
        states = []
        actions_taken = []
        rewards = []
        
        done = False
        steps = 0
        
        while not done and steps < 500:
            states.append({
                'agent_pos': env.agent_pos.copy(),
                'target_pos': env.target_pos.copy(),
                'obstacles': [(obs[0], obs[1], obs[2]) for obs in env.obstacles]
            })
            
            action = agent.predict(obs, deterministic=True)
            actions_taken.append(action.copy())
            
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            rewards.append(reward)
            steps += 1
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        states.append({
            'agent_pos': env.agent_pos.copy(),
            'target_pos': env.target_pos.copy(),
            'obstacles': [(obs[0], obs[1], obs[2]) for obs in env.obstacles]
        })
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∏–º–∞—Ü–∏–∏
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        fig.suptitle(f'{title}\nAlgorithm: {algo_name.upper()}', 
                     fontsize=14, fontweight='bold')
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1)
        ax1.set_aspect('equal')
        ax1.set_xlabel('X Position')
        ax1.set_ylabel('Y Position')
        ax1.grid(True, alpha=0.3)
        
        # –†–∏—Å—É–µ–º –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è
        for obs_x, obs_y, obs_r in states[0]['obstacles']:
            circle = plt.Circle((obs_x, obs_y), obs_r, color='gray', alpha=0.3)
            ax1.add_patch(circle)
        
        # –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –∞–≥–µ–Ω—Ç–∞
        agent_traj, = ax1.plot([], [], 'b-', alpha=0.5, linewidth=1, label='Agent Path')
        # –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è —Ü–µ–ª–∏
        target_traj, = ax1.plot([], [], 'r--', alpha=0.5, linewidth=1, label='Target Path')
        # –¢–µ–∫—É—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏
        agent_point, = ax1.plot([], [], 'bo', markersize=10, label='Agent')
        target_point, = ax1.plot([], [], 'r*', markersize=15, label='Target')
        
        ax1.legend(loc='upper right')
        
        # –ì—Ä–∞—Ñ–∏–∫ –Ω–∞–≥—Ä–∞–¥—ã
        ax2.set_xlabel('Step')
        ax2.set_ylabel('Cumulative Reward')
        ax2.set_title('Reward Progress')
        ax2.grid(True, alpha=0.3)
        reward_line, = ax2.plot([], [], 'g-', linewidth=2)
        
        def init():
            agent_traj.set_data([], [])
            target_traj.set_data([], [])
            agent_point.set_data([], [])
            target_point.set_data([], [])
            reward_line.set_data([], [])
            return agent_traj, target_traj, agent_point, target_point, reward_line
        
        def animate(frame):
            if frame >= len(states):
                frame = len(states) - 1
            
            # –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –º–æ–º–µ–Ω—Ç–∞
            agent_x = [s['agent_pos'][0] for s in states[:frame+1]]
            agent_y = [s['agent_pos'][1] for s in states[:frame+1]]
            target_x = [s['target_pos'][0] for s in states[:frame+1]]
            target_y = [s['target_pos'][1] for s in states[:frame+1]]
            
            agent_traj.set_data(agent_x, agent_y)
            target_traj.set_data(target_x, target_y)
            
            # –¢–µ–∫—É—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏
            agent_point.set_data([agent_x[-1]], [agent_y[-1]])
            target_point.set_data([target_x[-1]], [target_y[-1]])
            
            # –ù–∞–≥—Ä–∞–¥–∞
            if frame < len(rewards):
                cum_rewards = np.cumsum(rewards[:frame+1])
                reward_line.set_data(range(len(cum_rewards)), cum_rewards)
                ax2.set_xlim(0, len(rewards))
                ax2.set_ylim(min(cum_rewards.min(), 0), max(cum_rewards.max(), 10))
            
            return agent_traj, target_traj, agent_point, target_point, reward_line
        
        anim = animation.FuncAnimation(fig, animate, init_func=init,
                                       frames=len(states), interval=50, blit=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        anim.save(str(output_file), writer='pillow', fps=20)
        plt.close()
        
        print(f"  ‚úÖ Saved animation: {output_file}")
    
    def create_all_animations(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∏–º–∞—Ü–∏–π –¥–ª—è –≤—Å–µ—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤"""
        for algo_name in self.models_info.keys():
            print(f"\n{'='*60}")
            print(f"Processing {algo_name.upper()}")
            print(f"{'='*60}")
            
            try:
                # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –∏ —Ö—É–¥—à–∏–π —Å—Ü–µ–Ω–∞—Ä–∏–∏
                best, worst = self.find_best_worst_scenarios(algo_name, n_scenarios=20)
                
                # –°–æ–∑–¥–∞–µ–º –∞–Ω–∏–º–∞—Ü–∏–∏
                best_file = self.output_dir / f'{algo_name}_best_case.gif'
                self.create_animation(algo_name, best['seed'], 
                                    f'Best Case - Reward: {best["reward"]:.2f}',
                                    best_file)
                
                worst_file = self.output_dir / f'{algo_name}_worst_case.gif'
                self.create_animation(algo_name, worst['seed'],
                                    f'Worst Case - Reward: {worst["reward"]:.2f}',
                                    worst_file)
                
            except Exception as e:
                print(f"‚ùå Error processing {algo_name}: {e}")
                continue


def main():
    parser = argparse.ArgumentParser(description="Create best/worst case animations")
    parser.add_argument('--models-info', type=str,
                       default='results/parallel_training_1m/20251024_001927/trained_models.json',
                       help='Path to trained_models.json')
    parser.add_argument('--output-dir', type=str,
                       default='results/animations',
                       help='Output directory for animations')
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö
    models_info_file = Path(args.models_info)
    if not models_info_file.exists():
        print(f"‚ùå Models info not found: {models_info_file}")
        print("Creating demo structure...")
        
        # –î–µ–º–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
        models_info = {
            'ppo': {
                'model_path': 'results/parallel_training_1m/20251024_001927/ppo/ppo_*/models/ppo_model.pth',
                'algo_config': 'configs/long_ppo.yaml'
            },
            'sac': {
                'model_path': 'results/parallel_training_1m/20251024_001927/sac/ppo_*/models/sac_model.pth',
                'algo_config': 'configs/long_sac.yaml'
            }
        }
    else:
        with open(models_info_file, 'r') as f:
            models_info = json.load(f)
    
    # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∏–º–∞—Ç–æ—Ä
    animator = BestWorstAnimator(models_info, output_dir)
    
    print("\nüé¨ Starting animation creation...")
    print(f"Output directory: {output_dir}")
    
    animator.create_all_animations()
    
    print(f"\n‚úÖ All animations created! Check {output_dir}")


if __name__ == "__main__":
    main()

